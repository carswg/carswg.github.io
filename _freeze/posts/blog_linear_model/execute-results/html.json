{
  "hash": "e43ffc99d69fa3ee6ad877db801be1ac",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Covariate Adjustment for Linear Models: Understanding FDA Advice on Standard Errors\"\ndesc: \"Covariate Adjustment for Linear Models\"\nauthor: \"ASA Covariate Adjustment Working Group Education Subteam\"\ndate: \"09/24/2024\"\ndraft: false\n---\n\n\n\n\n\nPlease see the website https://carswg.github.io/ for information about the ASA Covariate Adjustment Working Group.\n\n# Prior reading\n\nThe blogpost [How to get the most out of prognostic baseline variables in clinical trial](https://stats4datascience.com/posts/covariate_adjustment/) by Courtney Schiffman, Michel Friesenhahn, and Christina Rabe is an excellent starting point for learning about modern methods of covariate adjustment and their practical implementation. The post focuses on linear models and covers most important aspects.\n\nOne area that might benefit from further explanation is variance estimation. The post suggests \"using robust standard error estimators from the targeted maximum likelihood estimation (TMLE) literature\". Analytic formulas and R code are included in a straightforward way with references to the literature (Rosenblum and van der Laan, 2010). Nevertheless, it may be helpful to add some more intuitive explanation.  \n\n# Goal for this blogpost\n\nThe [FDA (2023) guidance document on covariate adjustment](https://www.fda.gov/regulatory-information/search-fda-guidance-documents/adjusting-covariates-randomized-clinical-trials-drugs-and-biological-products) contains the following paragraph in the section on linear models:\n\n**Nominal standard errors are often the default method in most statistical software packages. Even if the model is incorrectly specified, they are acceptable in two arm trials with 1:1 randomization. However, in other settings, these standard errors can be inaccurate when the model is misspecified. Therefore, the Agency recommends that sponsors consider use of a robust standard error method such as the Huber-White ???sandwich??? standard error when the model does not include treatment by covariate interactions (Rosenblum and van der Laan 2009; Lin 2013). Other robust standard error methods proposed in the literature can also cover cases with interactions (Ye et al. 2022). An appropriate nonparametric bootstrap procedure can also be used (Efron and Tibshirani 1993).**\n\nThere is a lot to unpick in this short paragraph. The situation can be summarized in the following two-by-two table of recommended variance estimation methods.\n\n\n![](./files_blog_linear_model/lm_two_by_two_ehw.PNG)\n\n\nThis is what we mean by these terms:\n\n- *Model-based* -- a.k.a. \"nominal\" standard errors. What is produced by default in `lm`. Assumes a linear model with homoscedastic error terms.\n- *Sandwich* -- a.k.a. Eicker-Huber-White standard errors.\n- *Influence function* -- variance estimation methods that are based on an estimator's [*influence function*](https://urldefense.com/v3/__https://economics.mit.edu/sites/default/files/publications/inf21_mar_27.pdf__;!!N3hqHg43uw!o5yA_o8iLzExQQ9Oxy2liAf_MXKGLGOE1oaPCaRgj7boCu766uTjubaaP9dpELk7H-MPn8bZq4Yfli9RPnTyH7amvn61NktOpg$) . See, for example, Tsiatis et al., (2008), Moore and van der Laan (2009), Rosenblum and van der Laan (2010), Ye et al. (2022).\n\nThe goal of this blogpost is to give an intuitive understanding for why this two-by-two table looks the way it does.\n\n\n# Assumptions about sampling framework and estimands\n\nThe two-by-two table makes assumptions about the sampling framework and target estimand. Namely, it assumes the superpopulation sampling framework with potential outcomes. That is, $(Y_i(1), Y_i(0), A_i, X_i)$ for $i=1,\\ldots,N$ are i.i.d. random variables from joint distribution $f$ where treatment assignment $A_i$ is independent of the covariate and potential outcomes $X_i, Y_i(0), Y_i(1)$. The target of inference is $E_f(Y_i(1) - Y_i(0))$. \nAlso we assume there is a sufficiently large sample size to rely on asymptotic results.\n\nIn this blogpost, we will restrict attention to simple random sampling. Many clinical trials employ permuted block randomization, which would contradict the assumption we have just made that $(Y_i(1), Y_i(0), A_i, X_i)$ are i.i.d. While it's possible to accommodate alternative randomization schemes (see e.g., Ye et al., 2022; Wang et al., 2023), we'll stick to the simpler theory around simple random sampling. For justification, see the section on [\"Accounting for Stratified Randomization\"  by Schiffman, Friesenhahn and Rabe](https://stats4datascience.com/posts/covariate_adjustment/#accounting-for-stratified-randomization) \n\n\nOne potential alternative estimand would be the Conditional Average Treatment Effect at one specific covariate value, CATE($x$), which is defined as $E_f(Y_i(1) - Y_i(0) \\mid X_i = x)$.  A common approach to estimating CATE($x$) is to assume a model of the form\n$$Y_i = \\beta_0 + \\beta_1A_i + \\beta_2X_i + \\epsilon_i,$$\nin which case CATE($x$) is equal to $\\beta_1$ for all $x$. So the task reduces to estimating $\\beta_1$. The Population Average Treatment Effect, $E_f(Y_i(1) - Y_i(0))$, is also equal to $\\beta_1$ in this case.\n\nIf the assumed model is wrong, the interpretation of $\\hat{\\beta}_1$ (the usual least squares estimator) as a consistent estimator of $E_f(Y_i(1) - Y_i(0) \\mid X_i = x)$ is wrong -- there is no reason in general for CATE($x$) to be constant across different values of $x$. On the other hand, the interpretation of $\\hat{\\beta}_1$ as a consistent estimator of $E_f(Y_i(1) - Y_i(0))$ remains correct (Van Lancker et al., 2024). This can be seen from the fact that $\\hat{\\beta}_1 \\approx \\bar{Y}_1 - \\bar{Y}_0 - \\hat{\\beta}_2(\\bar{X}_1 - \\bar{X}_0)$ where $\\bar{Y}_j$ and $\\bar{X}_j$ are the sample means of $Y$ and $X$, respectively, on treatment $j$ (see Appendix of Tsiatis et al., 2008), and $\\bar{X}_1 - \\bar{X}_0$ being asymptotically zero.\n\n\nSometimes, we may assume an outcome model that includes treatment-covariate interactions,\n$$Y_i = \\beta_0 + \\beta_1A_i + \\beta_2X_i + \\beta_3A_iX_i + \\epsilon_i.$$\nIn this case, if the true underlying model really does contain treatment-covariate interactions, then the estimand CATE($x$) is not constant in $x$, which makes it somewhat unsuitable as an estimand for primary analysis in an RCT. One could, however, with a few extra steps, use this model to estimate $E_f(Y_i(1) - Y_i(0))$.  One option is g-computation (or standardization), whereby we estimate $E_f(Y_i(1))$ by the average of the model based predictions when setting the treatment assignment to $1$,\n\n$$\\hat{\\mu}(1) = \\frac{1}{N} \\sum_{i = 1}^N \\hat{\\beta}_0 + \\hat{\\beta}_1 + \\hat{\\beta}_2X_i + \\hat{\\beta}_3X_i,$$\nand we estimate $E_f(Y_i(0))$ similarly by setting the treatment assignment to $0$, \n$$\\hat{\\mu}(0) = \\frac{1}{N} \\sum_{i = 1}^N \\hat{\\beta}_0 + \\hat{\\beta}_2X_i.$$\n\nA possible estimator of $E_f(Y_i(1) - Y_i(0))$ is therefore,\n\n$$\\hat{\\mu}(1) - \\hat{\\mu}(0) = \\hat{\\beta}_1 + \\bar{X}\\hat{\\beta}_3.$$\n\nIncluding treatment-covariate interaction terms in the outcome model is somewhat discouraged in [an EMA guidance document from 2015](https://www.ema.europa.eu/en/documents/scientific-guideline/guideline-adjustment-baseline-covariates-clinical-trials_en.pdf) with the following statement.\n\n***If there is no reason to suspect an interaction between treatment and a covariate then the primary analysis should only include the main effects for treatment and covariate.  Conversely, if a substantial treatment by covariate interaction is suspected at the design stage, then stratified randomisation and/or subgroup analyses should be pre-planned accordingly.***\n\nThe blogpost [How to get the most out of prognostic baseline variables in clinical trial](https://stats4datascience.com/posts/covariate_adjustment/) also recommends not to include such terms, but for a different reason. The experience of the authors is that adding such terms to a model does not typically lead to a meaningful increase in efficiency, and we need to be mindful of our \"model budget\", that is, the allowable number of terms in a working model relative to the sample size before performance degrades.\n\nFor a linear model with 1:1 randomization, under the sampling framework outlined above, there is no efficiency gain to be had from modelling treatment-covariate interactions even when these interactions are extremely large (Tsiatis, 2008). This possibly surprising fact deserves a separate blogpost.\n\nNevertheless, the FDA guidance document is open to including such treatment-covariate interaction terms:\n\n***The linear model may include treatment by covariate interaction terms. However, when using\nthis approach, the primary analysis can still be based on an estimate from the model of the\naverage treatment effect (Tsiatis et al. 2008; Ye et al. 2021).***\n\n\n# Understanding variance estimation recommendations in the 2x2 table.\n\n\n## Treatment by covariate interactions\n\nTo understand the right hand column of the two-by-two table, where neither the model-based nor the sandwich estimator would be recommended, we can stay within the framework of a correctly specified working model,\n$$Y_i = \\beta_0 + \\beta_1A_i + \\beta_2X_i + \\beta_3A_iX_i + \\epsilon_i,$$\n\nwhere $\\epsilon_i \\sim N(0, \\sigma^2)$. At the moment, we are assuming that this model is correctly specified  in the sense of having the correct *mean model* $E(Y_i\\mid A_i, X_i) = \\beta_0 + \\beta_1A_i + \\beta_2X_i + \\beta_3A_iX_i$, in the sense of having the correct *variance model* $\\mathrm{var}(Y_i\\mid A_i, X_i) = \\sigma^2$, and also in the sense of having the correct distribution. These assumptions will be relaxed later in the post.\n\n\nThe first important point is that the parameter $\\beta_1$ does not in general correspond to $E_f(Y_i(1) - Y_i(0))$, so we cannot simply report the maximum likelihood estimate $\\hat{\\beta}_1$.  To use the working model to estimate $E_f(Y_i(1) - Y_i(0))$, one option is g-computation (or standardization). As described above, this produces an estimator $\\hat{\\beta}_1 + \\bar{X}\\hat{\\beta}_3$. \n\nThe question remains how do we estimate $\\mathrm{var}(\\hat{\\beta}_1 + \\bar{X}\\hat{\\beta}_3)$?\n\n\n### Law of total variance\n\nThe law of total variance (with a correctly specified mean model) says that\n\n$$\\begin{equation} \n\\begin{split}\n\\mathrm{var}(\\hat{\\beta}_1 + \\bar{X}\\hat{\\beta}_3) &=  E \\left\\lbrace \\mathrm{var}(\\hat{\\beta}_1 + \\bar{X}\\hat{\\beta}_3 \\mid \\underline{A}, \\underline{X})\\right\\rbrace + \\mathrm{var}\\left\\lbrace E(\\hat{\\beta}_1 + \\bar{X}\\hat{\\beta}_3\\mid \\underline{A}, \\underline{X}) \\right\\rbrace\\\\  &=  \nE \\left\\lbrace \\mathrm{var}(\\hat{\\beta}_1 + \\bar{X}\\hat{\\beta}_3 \\mid \\underline{A}, \\underline{X})\\right\\rbrace + \\beta_3^2\\mathrm{var}(\\bar{X})\n\\end{split}\n\\end{equation}$$\n\n\nGiven our assumptions about the correctness of the working model, the first term, $E\\left\\lbrace \\mathrm{var}(\\hat{\\beta}_1 + \\bar{X}\\hat{\\beta}_3 \\mid \\underline{A}, \\underline{X})\\right\\rbrace$,  can be consistently estimated via\n\n\n$$(0 ~~~1 ~~~0 ~~~\\bar{x}) \\widehat{\\mathrm{var}}(\\hat{\\beta} \\mid \\underline{A}= \\underline{a}, \\underline{X}= \\underline{x})  \\left( \\begin{array}{c} 0\\\\ 1 \\\\0 \\\\ \\bar{x} \\end{array} \\right),\\qquad\\qquad(\\mathrm{1})$$\nwhere $\\widehat{\\mathrm{var}}(\\hat{\\beta} \\mid \\underline{A}= \\underline{a}, \\underline{X}= \\underline{x})$ is either the standard model-based variance estimator for the maximum likelihood estimator of $\\beta$ (as would typically be produced by default in standard software) in which case (1) represents the \"model based\" variance estimator in the two-by-two table above, or  $\\widehat{\\mathrm{var}}(\\hat{\\beta} \\mid \\underline{A}= \\underline{a}, \\underline{X}= \\underline{x})$ is a sandwich estimator, in which case (1) represents the \"sandwich\" variance estimator in the two-by-two table. In either case, (1) ignores the second term in the law of total variance, $\\beta_3^2\\mathrm{var}(\\bar{X})$. This is the reason why it is not recommended. Alternative variance estimators based on influence functions, e.g., Tstiatis et al. (2008), Rosenblum and van der Laan (2010), Ye et al. (2022), do take account of this second term. \n\nWe did not need to talk about misspecified working models in order to demonstrate why the \"model-based\" and \"sandwich\" variance estimators are not recommended when the working model contains treatment by covariate interactions. But it is worth saying at this point that the influence function methods remain asymptotically valid when the mean and/or variance model is misspecified.\n\n### Illustrative example\n\nAs a demonstration, we would need to simulate multiple (1000, say) trials from the working model above where there is a strong treatment by covariate interaction. For each simulated trial, we would record the estimate of the treatment effect, as well as the estimated variance of the treatment effect estimator according to the three different methods.\n\nWe'll use the influence function approach as implemented in Ye et al. (2022) using the R package [{robincar}](https://cran.r-project.org/web/packages/RobinCar/index.html), noting this is the same as previous versions (e.g., Tstiatis et al. (2008), Rosenblum and van der Laan (2010)).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sandwich)\nlibrary(RobinCar)\n\nsim_one_trial <- function(n){\n\n  a <- rbinom(n, 1, 0.5)\n  x <- rnorm(n)\n  y <- 1 + a + x + a * x + rnorm(n)\n  dat <- data.frame(y=y,a=factor(a),x=x)\n  \n  \n  fit <- glm(y ~ a * x, data = dat)\n  \n  estimate <- c(t(c(0, 1, 0, mean(x))) %*% fit$coefficients)\n  model_based <- t(c(0, 1, 0, mean(x))) %*% vcov(fit) %*% c(0, 1, 0, mean(x))\n  sandwich <- t(c(0, 1, 0, mean(x))) %*% sandwich::vcovHC(fit) %*% c(0, 1, 0, mean(x))\n  ye <- RobinCar::robincar_linear(dat, \n                                  \"a\", \n                                  \"y\", \n                                  covariate_cols = \"x\", \n                                  adj_method = \"ANHECOVA\", \n                                  contrast_h = \"diff\")$contrast$varcov[1,1]\n  \n  data.frame(estimate = estimate,\n             model_based = model_based, \n             sandwich = sandwich, \n             ye = ye)\n  \n}\n\nresults <- purrr::map_df(rep(200, 1e3), sim_one_trial)\n```\n:::\n\n\nEmpirically, the variance of the point estimates is...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvar(results$estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02440746\n```\n\n\n:::\n:::\n\n\nWe can see that the Ye et al.(2022) method is reasonably consistent with this...\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(results$ye)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02503825\n```\n\n\n:::\n:::\n\n\n...while this is not the case for the other two methods...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(results$model_based)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02022407\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(results$sandwich)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0206597\n```\n\n\n:::\n:::\n\n\n\n## Homogeneous working model\n\n\nWe can now move on to the first column of the two-by-two table. In this case, suppose the working model is\n$$Y_i = \\beta_0 + \\beta_1A_i + \\beta_2X_i + \\epsilon_i$$\nwith $N(0, \\sigma^2)$. \n\n\n\n\nIf the mean model is correctly specified then it is clear that $\\beta_1 = E_f(Y_i(1) - Y_i(0))$.  [Although it can also be shown via a lot of algebra (see e.g. the Appendix of Tsiatis et al. (2008)) that the maximum likelihood estimator $\\hat{\\beta}_1$ is consistent for $E_f(Y_i(1) - Y_i(0))$ even when the model is completely misspecified]. \n\nThe question now is how to estimate $\\mathrm{var}(\\hat{\\beta}_1)$? \n\n\nIn contrast to the treatment-covariate interaction case, the law of total variance (with an assumption of a correctly specified mean model) now tells us that we can focus on estimating the conditional variance,\n$$\n\\begin{equation} \n\\begin{split}\n\\mathrm{var}(\\hat{\\beta}_1) &=  E \\left\\lbrace \\mathrm{var}(\\hat{\\beta}_1  \\mid \\underline{A}, \\underline{X})\\right\\rbrace + \\mathrm{var}\\left\\lbrace E(\\hat{\\beta}_1\\mid \\underline{A}, \\underline{X}) \\right\\rbrace\\\\  &=  \nE \\left\\lbrace \\mathrm{var}(\\hat{\\beta}_1  \\mid \\underline{A}, \\underline{X})\\right\\rbrace + 0.\n\\end{split}\n\\end{equation}\n$$\n\nThere are two options: 1) a completely model-based approach, $\\widehat{\\mathrm{var}}_M(\\hat{\\beta}_1 \\mid \\underline{A}= \\underline{a}, \\underline{X}= \\underline{x})$, as would typically be produced by default in standard software. Or, 2) the well known Huber-White (White, 1980) \"sandwich\" estimator, $\\widehat{\\mathrm{var}}_S(\\hat{\\beta}_1 \\mid \\underline{A}= \\underline{a}, \\underline{X}= \\underline{x})$.\n\nIf all the working model assumptions are satisfied, then both (1) and (2) are consistent estimators of $\\mathrm{var}(\\hat{\\beta}_1)$. We could put a provisional tick mark next to all methods in the left-hand column of the two-by-two table. To replace some of those ticks with crosses, we can consider what happens when the variance model in the working model is not correctly specified.\n\n### Back to an unadjusted analysis\n\nActually, we can make things even simpler for ourselves by considering an unadjusted analysis with working model\n$$Y_i = \\beta_0 + \\beta_1A_i + \\epsilon_i, \\qquad \\epsilon_i \\sim N(0, \\sigma^2)$$\nbut where the true model is\n$$Y_i = \\beta_0 + \\beta_1A_i + \\epsilon_i, \\qquad \\epsilon_i \\sim \\begin{array}{c} N(0, \\sigma_0^2), ~~A_i = 0 \\\\N(0, \\sigma_1^2), ~~A_i = 1 \\end{array}.$$\nWhen using the misspecified working model, the maximum likelihood estimator for $\\mathrm{var}(\\hat{\\beta}_1 \\mid \\underline{A} = \\underline{a})$ is\n\n$$v^{\\mathrm{miss}} =\\frac{\\hat{\\sigma}^2}{n_0} + \\frac{\\hat{\\sigma}^2}{n_1}.$$\n\nSubstituting in \n\n$$ \\hat{\\sigma}^2 = \\frac{n_0\\hat{\\sigma}_0^2 + n_1\\hat{\\sigma}_1^2}{n_0 + n_1}$$\nand rearranging yields\n\n$$v^{\\mathrm{miss}} = \\frac{\\hat{\\sigma}_0^2}{n_1} + \\frac{\\hat{\\sigma}_1^2}{n_0},$$\nwhereas when using the correct true model, the MLE based variance estimator is \n$$v^{\\mathrm{true}} =\\frac{\\hat{\\sigma}_0^2}{n_0} + \\frac{\\hat{\\sigma}_1^2}{n_1}.$$\nObviously the two are equivalent under equal allocation, but this is not the case for unequal allocation. We can plot what happens to $v^{\\mathrm{miss}}  / v^{\\mathrm{true}}$ as a function of $\\sigma_0 / \\sigma_1$ for various choices of allocation ratio (Treatment 1: Treatment 0), e.g, 2:1, 1:1 and 1:2 randomization...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsigma_1 <- 1\nn_1 <- 100\nsigma_0 <- seq(0.33, 2, length = 100)\nv_ratio <- function(n_0){(sigma_0^2/n_1+sigma_1^2/n_0)/(sigma_1^2/n_1+sigma_0^2/n_0)}\nplot(sigma_0, v_ratio(n_0 = 50), type = \"l\", xlab = \"sigma_0 / sigma_1\", ylab = \"ratio of variance estimates\")\npoints(sigma_0, v_ratio(n_0 = 100), type = \"l\", col = 2)\npoints(sigma_0, v_ratio(n_0 = 200), type = \"l\", col = 3)\nlegend(1,1.6,c(\"2:1\", \"1:1\", \"1:2\"), lty = c(1,1,1), col = 1:3)\n```\n\n::: {.cell-output-display}\n![](blog_linear_model_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\nThis shows, for example, that if the true variance on treatment 0 is larger than the true variance on treatment 1, and there are more patients on treatment 1, then the estimated model-based variance of $\\hat{\\beta}_1$ based on the misspecified working model will underestimate the true variance of $\\hat{\\beta}_1$.\n\nThis is the reason why we need to put a cross next to the model-based approach for non-1:1 randomization when using a homogeneous working model. But we can keep a provisional tick when using 1:1 randomization.\n\n### What about the sandwich estimator?\n\nFor an unadjusted analysis, the sandwich estimator for $\\mathrm{var}(\\hat{\\beta}_1)$ based on the misspecified working model is essentially equivalent to $v^{\\mathrm{true}}$ above. So for this particular example, the sandwich estimator would still work fine. We can keep a provisional tick mark next to the sandwich estimator for both 1:1 and non-1:1 randomization. \n\n\n# Summary and comment on misspecified mean models\n\nIn this blogpost, we have attempted to show why the two-by-two table of recommended variance estimators looks the way it does. \n\nWe have been able to demonstrate clearly the reasons for all of the crosses in the two-by-two table. The ticks, however, are still provisional. We have only considered correctly-specifed models and one specific example where the only source of misspecification is an equal variance assumption. \n\nIt turns out that the tick marks can stay even when the working mean model is misspecified (subject to some regularity conditions). Proving this level of robustness has been the subject of much impressive work (see the reference list below and [a nice summary here](https://thestatsgeek.com/2019/04/29/ancova-in-rcts-model-based-standard-errors-are-valid-even-under-misspecification/)) and is beyond the scope of this blogpost. \n\n\n# P.s. more alternative estimands\n\nThe above discussion takes for granted that the estimand is the unconditional expectation of the difference in sample means\n\n$E_f(Y_i(1) - Y_i(0)) = E_{f^N}(\\bar{Y}_1 - \\bar{Y}_0)$,\n\nwhere $f^N$ is the joint distribution of all $N$ individuals' data.\n\nAnother potential alternative estimand is the conditional expectation for the difference in sample means given the vector of baseline covariates,\n\n$\\frac{1}{N}\\sum_{i=1}^NE_f(Y_i(1) - Y_i(0) \\mid X_i = x_i) = E_{f^N}(\\bar{Y}_1 - \\bar{Y}_0 \\mid \\underline{X} = \\underline{x})$.\n\nThis alternative estimand is discussed in the context of binary outcomes by [Magirr et al. (2024)](https://doi.org/10.17605/OSF.IO/9MP58). In this case, the recommendations for variance estimation would need to be changed, to reflect that the vector of baseline covariates is considered fixed.\n\nYet another alternative estimand is the sample average treatment effect,\n\n$$\\frac{1}{N}\\sum_{i=1}^N Y_i(1) - Y_i(0),$$\n\nwhere *randomization inference* methods could be applied (Guo and Basse, 2023).\n\n\n\n# References\n\nEfron, B and RJ Tibshirani, 1993, An Introduction to the Bootstrap, Boca Raton (FL): Chapman & Hall. \n\nRosenblum, M and MJ van der Laan, 2009, Using Regression Models to Analyze Randomized Trials: Asymptotically Valid Hypothesis Tests Despite Incorrectly Specified Models, Biometrics, 65(3):937-945. \n\nLin W, 2013, Agnostic Notes on Regression Adjustments to Experimental Data: Reexamining Freedman???s Critique, Annals of Applied Statistics, 7(1):295-318.\n\nYe, T, J Shao, Y Yi, and Q Zhao, 2022, Toward better practice Of Covariate Adjustment In Analyzing Randomized Clinical Trials, Journal of the American Statistical Association, doi: 10.1080/01621459.2022.2049278. \n\nTsiatis, Anastasios A., et al. \"Covariate adjustment for two???sample treatment comparisons in randomized clinical trials: a principled yet flexible approach.\" Statistics in medicine 27.23 (2008): 4658-4677.\n\nWhite, Halbert. \"A heteroskedasticity-consistent covariance matrix estimator and a direct test for heteroskedasticity.\" Econometrica: journal of the Econometric Society (1980): 817-838.\n\nVan Lancker, Kelly, Frank Bretz, and Oliver Dukes. \"Covariate adjustment in randomized controlled trials: General concepts and practical considerations.\" Clinical Trials (2024): 17407745241251568.\n\nGuo, Kevin, and Guillaume Basse. \"The generalized oaxaca-blinder estimator.\" Journal of the American Statistical Association 118.541 (2023): 524-536.\n\nWang, B., Susukida, R., Mojtabai, R., Amin-Esmaeili, M., & Rosenblum, M. (2023). Model-robust inference for clinical trials that improve precision by stratified randomization and covariate adjustment. Journal of the American Statistical Association, 118(542), 1152-1163\n\n\n```{=html}\n<script src=\"https://giscus.app/client.js\"\n        data-repo=\"carswg/carswg.github.io\"\n        data-repo-id=\"R_kgDOLX8xOg\"\n        data-category=\"General\"\n        data-category-id=\"DIC_kwDOLX8xOs4CeZQB\"\n        data-mapping=\"title\"\n        data-strict=\"1\"\n        data-reactions-enabled=\"1\"\n        data-emit-metadata=\"0\"\n        data-input-position=\"bottom\"\n        data-theme=\"light_high_contrast\"\n        data-lang=\"en\"\n        crossorigin=\"anonymous\"\n        async>\n</script>\n```\n",
    "supporting": [
      "blog_linear_model_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}